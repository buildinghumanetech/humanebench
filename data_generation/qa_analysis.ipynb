{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Humane Bench QA Analysis\n",
    "\n",
    "This notebook provides comprehensive quality assurance and data science analysis for the Humane Bench dataset.\n",
    "\n",
    "**Contents:**\n",
    "1. Exploratory Data Analysis (EDA)\n",
    "2. Distribution Analysis & Visualization\n",
    "3. Stratified Random Sampling for Manual Review\n",
    "4. Inter-Rater Reliability Analysis (Fleiss' Kappa)\n",
    "\n",
    "**Usage:**\n",
    "- Run cells sequentially for full analysis\n",
    "- Section 3 exports a sample CSV for manual review\n",
    "- Section 4 analyzes inter-rater agreement after reviews are completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from typing import List, Dict\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "# Import constants from config\n",
    "from config import HUMANE_PRINCIPLES, TOPIC_DOMAINS, VULNERABLE_POPULATIONS, DATASET_PATH\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úì Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "def load_dataset(path: str = DATASET_PATH) -> pd.DataFrame:\n",
    "    \"\"\"Load JSONL dataset into a pandas DataFrame.\"\"\"\n",
    "    data = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Extract metadata fields into columns\n",
    "    df['principle'] = df['metadata'].apply(lambda x: x.get('principle', ''))\n",
    "    df['domain'] = df['metadata'].apply(lambda x: x.get('domain', ''))\n",
    "    df['vulnerable_population'] = df['metadata'].apply(lambda x: x.get('vulnerable-population', ''))\n",
    "    \n",
    "    # Calculate input length\n",
    "    df['input_length'] = df['input'].str.len()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load data\n",
    "df = load_dataset()\n",
    "\n",
    "print(f\"‚úì Loaded {len(df)} scenarios\")\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"Sample scenarios:\")\n",
    "df[['id', 'input', 'principle', 'domain', 'vulnerable_population']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"=== DATASET OVERVIEW ===\")\n",
    "print(f\"Total scenarios: {len(df)}\")\n",
    "print(f\"Unique principles: {df['principle'].nunique()}\")\n",
    "print(f\"Unique domains: {df['domain'].nunique()}\")\n",
    "print(f\"Scenarios with vulnerable populations: {df['vulnerable_population'].astype(bool).sum()} ({df['vulnerable_population'].astype(bool).sum()/len(df)*100:.1f}%)\")\n",
    "print(f\"\\nInput length: mean={df['input_length'].mean():.1f}, median={df['input_length'].median():.1f}, min={df['input_length'].min()}, max={df['input_length'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Distribution Analysis & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principle distribution\n",
    "principle_counts = df['principle'].value_counts()\n",
    "principle_pct = df['principle'].value_counts(normalize=True) * 100\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Count plot\n",
    "principle_counts.plot(kind='barh', ax=ax1, color='steelblue')\n",
    "ax1.set_title('Scenarios per Principle (Count)', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Count')\n",
    "ax1.set_ylabel('Principle')\n",
    "ax1.axvline(len(df) / len(HUMANE_PRINCIPLES), color='red', linestyle='--', label=f'Target (balanced): {len(df) / len(HUMANE_PRINCIPLES):.0f}')\n",
    "ax1.legend()\n",
    "\n",
    "# Percentage plot\n",
    "principle_pct.plot(kind='barh', ax=ax2, color='coral')\n",
    "ax2.set_title('Scenarios per Principle (Percentage)', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Percentage (%)')\n",
    "ax2.set_ylabel('')\n",
    "ax2.axvline(100 / len(HUMANE_PRINCIPLES), color='red', linestyle='--', label=f'Target: {100/len(HUMANE_PRINCIPLES):.1f}%')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed breakdown\n",
    "print(\"\\n=== PRINCIPLE DISTRIBUTION ===\")\n",
    "target_pct = 100.0 / len(HUMANE_PRINCIPLES)\n",
    "for principle in HUMANE_PRINCIPLES:\n",
    "    count = principle_counts.get(principle, 0)\n",
    "    pct = principle_pct.get(principle, 0.0)\n",
    "    deviation = abs(pct - target_pct)\n",
    "    status = \"‚úì\" if deviation < 5.0 else \"‚ö†\"\n",
    "    print(f\"{status} {principle:40s} {count:4d} ({pct:5.1f}%) [deviation: {deviation:4.1f}%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain distribution\n",
    "domain_counts = df['domain'].value_counts().head(15)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "domain_counts.plot(kind='barh', color='mediumseagreen')\n",
    "plt.title('Top 15 Domains (Count)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Domain')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n=== DOMAIN DISTRIBUTION ===\")\n",
    "print(f\"Total unique domains: {df['domain'].nunique()}\")\n",
    "print(f\"\\nTop 15 domains:\")\n",
    "for domain, count in domain_counts.items():\n",
    "    pct = count / len(df) * 100\n",
    "    in_standard = \"‚úì\" if domain in TOPIC_DOMAINS else \"‚ö†\"\n",
    "    print(f\"{in_standard} {domain:30s} {count:4d} ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vulnerable population distribution\n",
    "vuln_pop_counts = df[df['vulnerable_population'] != '']['vulnerable_population'].value_counts()\n",
    "\n",
    "if len(vuln_pop_counts) > 0:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    vuln_pop_counts.plot(kind='bar', color='indianred')\n",
    "    plt.title('Vulnerable Population Distribution', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Population Type')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "total_with_vuln = (df['vulnerable_population'] != '').sum()\n",
    "total_without_vuln = len(df) - total_with_vuln\n",
    "\n",
    "print(f\"\\n=== VULNERABLE POPULATION DISTRIBUTION ===\")\n",
    "print(f\"With vulnerable population: {total_with_vuln} ({total_with_vuln/len(df)*100:.1f}%)\")\n",
    "print(f\"Without: {total_without_vuln} ({total_without_vuln/len(df)*100:.1f}%)\")\n",
    "print(f\"\\nBreakdown:\")\n",
    "for pop, count in vuln_pop_counts.items():\n",
    "    pct = count / len(df) * 100\n",
    "    print(f\"  {pop:25s} {count:4d} ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap: Principle x Domain coverage\n",
    "crosstab = pd.crosstab(df['principle'], df['domain'])\n",
    "\n",
    "# Show top domains only for readability\n",
    "top_domains = df['domain'].value_counts().head(12).index\n",
    "crosstab_subset = crosstab[top_domains]\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(crosstab_subset, annot=True, fmt='d', cmap='YlOrRd', cbar_kws={'label': 'Count'})\n",
    "plt.title('Principle √ó Domain Coverage (Top 12 Domains)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Domain')\n",
    "plt.ylabel('Principle')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input length distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Histogram\n",
    "ax1.hist(df['input_length'], bins=50, color='skyblue', edgecolor='black')\n",
    "ax1.axvline(df['input_length'].mean(), color='red', linestyle='--', label=f'Mean: {df[\"input_length\"].mean():.1f}')\n",
    "ax1.axvline(df['input_length'].median(), color='green', linestyle='--', label=f'Median: {df[\"input_length\"].median():.1f}')\n",
    "ax1.set_title('Input Length Distribution', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Character Count')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.legend()\n",
    "\n",
    "# Box plot by principle\n",
    "df.boxplot(column='input_length', by='principle', ax=ax2, rot=90)\n",
    "ax2.set_title('Input Length by Principle', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Principle')\n",
    "ax2.set_ylabel('Character Count')\n",
    "plt.suptitle('')  # Remove default title\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== INPUT LENGTH STATISTICS ===\")\n",
    "print(df['input_length'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Mode Collapse Detection - Within-Principle Similarity\n",
    "\n",
    "Following the DarkBench approach, this section checks for **high cosine similarity within each principle** to ensure heterogeneity and avoid mode collapse.\n",
    "\n",
    "**What we're checking:**\n",
    "- Cosine similarity between scenario pairs **within each principle** (not across principles)\n",
    "- This ensures that scenarios for each principle are sufficiently diverse\n",
    "- Prevents mode collapse where models give similar responses to all scenarios in a category\n",
    "\n",
    "**Why this matters:**\n",
    "- If scenarios within a principle are too similar, models may learn to respond uniformly\n",
    "- High within-category similarity indicates potential for mode collapse\n",
    "- DarkBench uses this same approach to validate benchmark quality\n",
    "\n",
    "**Methodology:**\n",
    "- Sample ~70 scenarios per principle (or all if fewer)\n",
    "- Generate embeddings using `text-embedding-3-large`\n",
    "- Compute all pairwise cosine similarities within each principle\n",
    "- Flag pairs with similarity ‚â•0.60 as concerning\n",
    "\n",
    "**Threshold:** Similarities ‚â•0.60 indicate scenarios that may be too similar and could lead to mode collapse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and embed scenarios for mode collapse detection\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "dotenv_path = Path.cwd() / '.env'\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "def get_embeddings(texts: List[str], model: str = \"text-embedding-3-large\", batch_size: int = 100) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Get embeddings for texts using OpenAI's embedding API.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text strings to embed\n",
    "        model: Embedding model to use\n",
    "        batch_size: Number of texts to process at once\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of embeddings (n_texts, embedding_dim)\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        try:\n",
    "            response = client.embeddings.create(\n",
    "                model=model,\n",
    "                input=batch\n",
    "            )\n",
    "            embeddings = [item.embedding for item in response.data]\n",
    "            all_embeddings.extend(embeddings)\n",
    "            \n",
    "            # Rate limiting\n",
    "            if i + batch_size < len(texts):\n",
    "                time.sleep(0.5)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch {i//batch_size + 1}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    return np.array(all_embeddings)\n",
    "\n",
    "print(\"‚úì OpenAI client initialized\")\n",
    "\n",
    "# Sample scenarios for each principle\n",
    "SCENARIOS_PER_PRINCIPLE = 100\n",
    "\n",
    "print(f\"\\n=== SAMPLING SCENARIOS ===\")\n",
    "scenario_texts = []\n",
    "scenario_principles = []\n",
    "\n",
    "for principle in HUMANE_PRINCIPLES:\n",
    "    principle_df = df[df['principle'] == principle]\n",
    "    n_samples = min(SCENARIOS_PER_PRINCIPLE, len(principle_df))\n",
    "    sample = principle_df.sample(n=n_samples, random_state=42)\n",
    "    \n",
    "    scenario_texts.extend(sample['input'].tolist())\n",
    "    scenario_principles.extend([principle] * n_samples)\n",
    "    \n",
    "    print(f\"  {principle}: {n_samples} scenarios\")\n",
    "\n",
    "print(f\"\\nTotal scenarios: {len(scenario_texts)}\")\n",
    "\n",
    "# Generate embeddings\n",
    "print(f\"\\n=== GENERATING EMBEDDINGS ===\")\n",
    "print(\"This may take a minute...\")\n",
    "\n",
    "scenario_embeddings = get_embeddings(scenario_texts)\n",
    "\n",
    "print(f\"‚úì Generated {len(scenario_embeddings)} embeddings (dimension: {scenario_embeddings.shape[1]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute within-principle pairwise similarities\n",
    "print(\"=== COMPUTING WITHIN-PRINCIPLE SIMILARITIES ===\\n\")\n",
    "\n",
    "HIGH_SIM_THRESHOLD = 0.40\n",
    "\n",
    "within_principle_results = {}\n",
    "\n",
    "for principle in HUMANE_PRINCIPLES:\n",
    "    print(f\"Processing {principle}...\")\n",
    "    \n",
    "    # Get scenarios and embeddings for this principle\n",
    "    principle_mask = np.array(scenario_principles) == principle\n",
    "    principle_embeddings = scenario_embeddings[principle_mask]\n",
    "    principle_texts = [scenario_texts[i] for i in range(len(scenario_texts)) if scenario_principles[i] == principle]\n",
    "    \n",
    "    n_scenarios = len(principle_texts)\n",
    "    \n",
    "    if n_scenarios < 2:\n",
    "        print(f\"  Skipping (only {n_scenarios} scenario)\")\n",
    "        continue\n",
    "    \n",
    "    # Compute pairwise similarities\n",
    "    sim_matrix = cosine_similarity(principle_embeddings)\n",
    "    \n",
    "    # Extract upper triangle (excluding diagonal)\n",
    "    similarities = []\n",
    "    high_sim_pairs = []\n",
    "    all_pairs = []\n",
    "    \n",
    "    for i in range(n_scenarios):\n",
    "        for j in range(i + 1, n_scenarios):\n",
    "            sim = sim_matrix[i, j]\n",
    "            similarities.append(sim)\n",
    "            pair_info = (i, j, sim, principle_texts[i], principle_texts[j])\n",
    "            all_pairs.append(pair_info)\n",
    "            \n",
    "            if sim >= HIGH_SIM_THRESHOLD:\n",
    "                high_sim_pairs.append(pair_info)\n",
    "    \n",
    "    # Sort pairs by similarity\n",
    "    all_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "    high_sim_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    # Get lowest similarity pairs (most different)\n",
    "    low_sim_pairs = sorted(all_pairs, key=lambda x: x[2])[:5]\n",
    "    \n",
    "    # Store results\n",
    "    similarities = np.array(similarities)\n",
    "    within_principle_results[principle] = {\n",
    "        'n_scenarios': n_scenarios,\n",
    "        'n_pairs': len(similarities),\n",
    "        'similarities': similarities,\n",
    "        'min': similarities.min(),\n",
    "        'max': similarities.max(),\n",
    "        'mean': similarities.mean(),\n",
    "        'std': similarities.std(),\n",
    "        'n_high_sim': len(high_sim_pairs),\n",
    "        'high_sim_pairs': high_sim_pairs,\n",
    "        'low_sim_pairs': low_sim_pairs\n",
    "    }\n",
    "    \n",
    "    print(f\"  {n_scenarios} scenarios, {len(similarities)} pairs, {len(high_sim_pairs)} high-similarity (‚â•{HIGH_SIM_THRESHOLD})\")\n",
    "\n",
    "print(f\"\\n‚úì Computed similarities for all {len(within_principle_results)} principles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results per principle\n",
    "print(\"=== WITHIN-PRINCIPLE SIMILARITY RESULTS ===\\n\")\n",
    "\n",
    "for principle in HUMANE_PRINCIPLES:\n",
    "    if principle not in within_principle_results:\n",
    "        continue\n",
    "    \n",
    "    results = within_principle_results[principle]\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"PRINCIPLE: {principle}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    print(f\"\\nStatistics:\")\n",
    "    print(f\"  Scenarios analyzed: {results['n_scenarios']}\")\n",
    "    print(f\"  Total pairwise comparisons: {results['n_pairs']}\")\n",
    "    print(f\"  Min similarity: {results['min']:.4f}\")\n",
    "    print(f\"  Mean similarity: {results['mean']:.4f}\")\n",
    "    print(f\"  Max similarity: {results['max']:.4f}\")\n",
    "    print(f\"  Std deviation: {results['std']:.4f}\")\n",
    "    print(f\"  High similarity pairs (‚â•0.60): {results['n_high_sim']} ({results['n_high_sim']/results['n_pairs']*100:.1f}%)\")\n",
    "    \n",
    "    # Show high similarity pairs\n",
    "    if results['n_high_sim'] > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  TOP 5 MOST SIMILAR PAIRS (HIGH CONCERN):\")\n",
    "        for i, (idx1, idx2, sim, text1, text2) in enumerate(results['high_sim_pairs'][:5], 1):\n",
    "            print(f\"\\n  {i}. Similarity: {sim:.4f}\")\n",
    "            print(f\"     Scenario A: {text1[:80]}...\")\n",
    "            print(f\"     Scenario B: {text2[:80]}...\")\n",
    "    else:\n",
    "        print(f\"\\n‚úì No high similarity pairs found (all < 0.60)\")\n",
    "    \n",
    "    # Show lowest similarity pairs for context\n",
    "    print(f\"\\nüìä TOP 5 MOST DIFFERENT PAIRS (GOOD DIVERSITY):\")\n",
    "    for i, (idx1, idx2, sim, text1, text2) in enumerate(results['low_sim_pairs'][:5], 1):\n",
    "        print(f\"\\n  {i}. Similarity: {sim:.4f}\")\n",
    "        print(f\"     Scenario A: {text1[:80]}...\")\n",
    "        print(f\"     Scenario B: {text2[:80]}...\")\n",
    "\n",
    "print(f\"\\n{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary visualization and recommendations\n",
    "print(\"=== SUMMARY: MODE COLLAPSE DETECTION ===\\n\")\n",
    "\n",
    "# Summary statistics table\n",
    "summary_data = []\n",
    "for principle in HUMANE_PRINCIPLES:\n",
    "    if principle in within_principle_results:\n",
    "        stats = within_principle_results[principle]\n",
    "        summary_data.append({\n",
    "            'Principle': principle,\n",
    "            'Min': f\"{stats['min']:.3f}\",\n",
    "            'Mean': f\"{stats['mean']:.3f}\",\n",
    "            'Max': f\"{stats['max']:.3f}\",\n",
    "            'Std': f\"{stats['std']:.3f}\",\n",
    "            'High Sim Pairs (‚â•0.60)': stats['n_high_sim']\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Visualizations\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Box plot of similarity distributions\n",
    "similarity_dists = []\n",
    "labels = []\n",
    "for principle in HUMANE_PRINCIPLES:\n",
    "    if principle in within_principle_results:\n",
    "        similarity_dists.append(within_principle_results[principle]['similarities'])\n",
    "        labels.append(principle.replace('-', '\\n'))\n",
    "\n",
    "bp = ax1.boxplot(similarity_dists, labels=labels, patch_artist=True)\n",
    "for patch in bp['boxes']:\n",
    "    patch.set_facecolor('lightblue')\n",
    "\n",
    "ax1.axhline(0.60, color='red', linestyle='--', alpha=0.7, label='Threshold (0.60)')\n",
    "ax1.set_ylabel('Cosine Similarity')\n",
    "ax1.set_title('Within-Principle Similarity Distributions', fontweight='bold', fontsize=12)\n",
    "ax1.set_xticklabels(labels, rotation=45, ha='right', fontsize=9)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Bar chart of high similarity pairs\n",
    "high_sim_counts = [within_principle_results[p]['n_high_sim'] for p in HUMANE_PRINCIPLES if p in within_principle_results]\n",
    "principle_labels = [p.replace('-', '\\n') for p in HUMANE_PRINCIPLES if p in within_principle_results]\n",
    "\n",
    "colors = ['red' if count > 10 else 'orange' if count > 5 else 'green' for count in high_sim_counts]\n",
    "bars = ax2.bar(principle_labels, high_sim_counts, color=colors, alpha=0.7)\n",
    "ax2.set_ylabel('Number of High Similarity Pairs')\n",
    "ax2.set_title('High Similarity Pairs (‚â•0.60) by Principle', fontweight='bold', fontsize=12)\n",
    "ax2.set_xticklabels(principle_labels, rotation=45, ha='right', fontsize=9)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{int(height)}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Recommendations\n",
    "print(\"\\n=== RECOMMENDATIONS ===\\n\")\n",
    "total_high_sim = sum(high_sim_counts)\n",
    "if total_high_sim == 0:\n",
    "    print(\"‚úì No concerning high similarities detected within any principle.\")\n",
    "    print(\"  Scenarios within each principle appear sufficiently heterogeneous.\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Found {total_high_sim} high similarity pairs (‚â•0.60) across all principles.\\n\")\n",
    "    print(\"Principles requiring attention:\")\n",
    "    for principle, count in zip(HUMANE_PRINCIPLES, high_sim_counts):\n",
    "        if count > 0:\n",
    "            print(f\"  ‚Ä¢ {principle}: {count} high-similarity pairs\")\n",
    "            if count > 10:\n",
    "                print(f\"    ‚Üí Consider generating more diverse scenarios for this principle\")\n",
    "    print(\"\\nReview the detailed results above to identify specific similar scenario pairs.\")\n",
    "\n",
    "print(\"\\n‚úì Mode collapse detection complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stratified Random Sampling for Manual Review\n",
    "\n",
    "Generate a stratified random sample ensuring representation across all principles and domains.\n",
    "\n",
    "**Sample Size:** ~10-15% of dataset (100-150 scenarios) provides 95% confidence with ~10% margin of error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_sample(df: pd.DataFrame, sample_size: int = 120, random_state: int = 42) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate stratified random sample ensuring representation across principles.\n",
    "    \n",
    "    Args:\n",
    "        df: Full dataset\n",
    "        sample_size: Target sample size (default: 120, ~11.5% of 1039)\n",
    "        random_state: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        Stratified sample DataFrame\n",
    "    \"\"\"\n",
    "    # Calculate samples per principle (proportional stratified sampling)\n",
    "    principle_counts = df['principle'].value_counts()\n",
    "    \n",
    "    samples_per_principle = {}\n",
    "    for principle, count in principle_counts.items():\n",
    "        proportion = count / len(df)\n",
    "        n_samples = max(int(sample_size * proportion), 1)  # At least 1 per principle\n",
    "        samples_per_principle[principle] = min(n_samples, count)  # Can't sample more than available\n",
    "    \n",
    "    # Adjust to hit target sample size\n",
    "    total_samples = sum(samples_per_principle.values())\n",
    "    if total_samples < sample_size:\n",
    "        # Distribute remaining samples proportionally\n",
    "        remaining = sample_size - total_samples\n",
    "        sorted_principles = sorted(principle_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        for principle, count in sorted_principles:\n",
    "            if remaining <= 0:\n",
    "                break\n",
    "            if samples_per_principle[principle] < count:\n",
    "                samples_per_principle[principle] += 1\n",
    "                remaining -= 1\n",
    "    \n",
    "    print(\"=== STRATIFIED SAMPLING PLAN ===\")\n",
    "    print(f\"Target sample size: {sample_size}\")\n",
    "    print(f\"\\nSamples per principle:\")\n",
    "    for principle in HUMANE_PRINCIPLES:\n",
    "        if principle in samples_per_principle:\n",
    "            n = samples_per_principle[principle]\n",
    "            total = principle_counts[principle]\n",
    "            pct = n / total * 100\n",
    "            print(f\"  {principle:40s} {n:3d} / {total:4d} ({pct:5.1f}%)\")\n",
    "    \n",
    "    # Perform stratified sampling\n",
    "    sample_dfs = []\n",
    "    for principle, n_samples in samples_per_principle.items():\n",
    "        principle_df = df[df['principle'] == principle]\n",
    "        sample_df = principle_df.sample(n=n_samples, random_state=random_state)\n",
    "        sample_dfs.append(sample_df)\n",
    "    \n",
    "    sample = pd.concat(sample_dfs, ignore_index=True)\n",
    "    \n",
    "    # Shuffle the sample\n",
    "    sample = sample.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\n‚úì Generated stratified sample of {len(sample)} scenarios\")\n",
    "    \n",
    "    return sample\n",
    "\n",
    "# Generate sample (adjust sample_size as needed)\n",
    "SAMPLE_SIZE = 120  # ~11.5% of 1039 scenarios\n",
    "sample_df = stratified_sample(df, sample_size=SAMPLE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify sample distribution\n",
    "print(\"\\n=== SAMPLE DISTRIBUTION VERIFICATION ===\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Compare principle distribution\n",
    "full_dist = df['principle'].value_counts(normalize=True).sort_index() * 100\n",
    "sample_dist = sample_df['principle'].value_counts(normalize=True).sort_index() * 100\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Full Dataset': full_dist,\n",
    "    'Sample': sample_dist\n",
    "})\n",
    "\n",
    "comparison_df.plot(kind='bar', ax=axes[0], color=['steelblue', 'coral'])\n",
    "axes[0].set_title('Principle Distribution: Full Dataset vs Sample', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Principle')\n",
    "axes[0].set_ylabel('Percentage (%)')\n",
    "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=45, ha='right')\n",
    "axes[0].legend()\n",
    "\n",
    "# Compare domain distribution (top 10)\n",
    "full_domain_dist = df['domain'].value_counts(normalize=True).head(10) * 100\n",
    "sample_domain_dist = sample_df['domain'].value_counts(normalize=True).head(10) * 100\n",
    "\n",
    "domain_comparison_df = pd.DataFrame({\n",
    "    'Full Dataset': full_domain_dist,\n",
    "    'Sample': sample_domain_dist\n",
    "}).fillna(0)\n",
    "\n",
    "domain_comparison_df.plot(kind='bar', ax=axes[1], color=['mediumseagreen', 'lightcoral'])\n",
    "axes[1].set_title('Domain Distribution: Full Dataset vs Sample (Top 10)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Domain')\n",
    "axes[1].set_ylabel('Percentage (%)')\n",
    "axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=45, ha='right')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export sample for manual review\n",
    "review_columns = ['id', 'input', 'target', 'principle', 'domain', 'vulnerable_population', 'input_length']\n",
    "review_sample = sample_df[review_columns].copy()\n",
    "\n",
    "# Add columns for reviewer ratings\n",
    "# Reviewers will code each scenario on multiple criteria\n",
    "review_sample['reviewer_1_clear'] = ''  # Is input clear? (yes/no)\n",
    "review_sample['reviewer_1_realistic'] = ''  # Is scenario realistic? (yes/no)\n",
    "review_sample['reviewer_1_tests_principle'] = ''  # Does it test target principle? (yes/no)\n",
    "review_sample['reviewer_1_quality'] = ''  # Overall quality (low/medium/high)\n",
    "review_sample['reviewer_1_notes'] = ''  # Optional notes\n",
    "\n",
    "review_sample['reviewer_2_clear'] = ''\n",
    "review_sample['reviewer_2_realistic'] = ''\n",
    "review_sample['reviewer_2_tests_principle'] = ''\n",
    "review_sample['reviewer_2_quality'] = ''\n",
    "review_sample['reviewer_2_notes'] = ''\n",
    "\n",
    "review_sample['reviewer_3_clear'] = ''\n",
    "review_sample['reviewer_3_realistic'] = ''\n",
    "review_sample['reviewer_3_tests_principle'] = ''\n",
    "review_sample['reviewer_3_quality'] = ''\n",
    "review_sample['reviewer_3_notes'] = ''\n",
    "\n",
    "# Save to CSV\n",
    "output_file = 'review_sample.csv'\n",
    "review_sample.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"‚úì Exported {len(review_sample)} scenarios to '{output_file}'\")\n",
    "print(f\"\\nReview instructions:\")\n",
    "print(\"1. Each reviewer should independently fill in their columns\")\n",
    "print(\"2. For yes/no fields: use 'yes' or 'no'\")\n",
    "print(\"3. For quality: use 'low', 'medium', or 'high'\")\n",
    "print(\"4. Notes are optional but helpful for discussing disagreements\")\n",
    "print(\"5. After all reviews are complete, return to this notebook for inter-rater reliability analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inter-Rater Reliability Analysis\n",
    "\n",
    "After manual reviews are completed, run this section to calculate **Fleiss' Kappa** - a statistical measure of inter-rater agreement.\n",
    "\n",
    "**Interpretation (Landis & Koch criteria):**\n",
    "- < 0.00: Poor agreement\n",
    "- 0.00-0.20: Slight agreement\n",
    "- 0.21-0.40: Fair agreement\n",
    "- 0.41-0.60: Moderate agreement\n",
    "- 0.61-0.80: Substantial agreement\n",
    "- 0.81-1.00: Almost perfect agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fleiss_kappa(ratings_matrix: np.ndarray, categories: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Fleiss' Kappa for inter-rater reliability.\n",
    "    \n",
    "    Args:\n",
    "        ratings_matrix: Matrix of shape (n_items, n_categories) where each cell\n",
    "                       contains the number of raters who assigned that category to that item\n",
    "        categories: Number of possible categories\n",
    "    \n",
    "    Returns:\n",
    "        Fleiss' Kappa coefficient\n",
    "    \"\"\"\n",
    "    n_items, n_categories = ratings_matrix.shape\n",
    "    n_raters = ratings_matrix.sum(axis=1)[0]  # Total raters per item (should be constant)\n",
    "    \n",
    "    # Calculate P_i (proportion of agreement for each item)\n",
    "    P_i = (np.sum(ratings_matrix ** 2, axis=1) - n_raters) / (n_raters * (n_raters - 1))\n",
    "    P_bar = np.mean(P_i)  # Mean proportion of agreement\n",
    "    \n",
    "    # Calculate P_j (proportion of all assignments to category j)\n",
    "    P_j = np.sum(ratings_matrix, axis=0) / (n_items * n_raters)\n",
    "    P_e_bar = np.sum(P_j ** 2)  # Expected proportion of agreement by chance\n",
    "    \n",
    "    # Calculate Fleiss' Kappa\n",
    "    if P_e_bar == 1.0:\n",
    "        return 1.0  # Perfect agreement\n",
    "    \n",
    "    kappa = (P_bar - P_e_bar) / (1 - P_e_bar)\n",
    "    return kappa\n",
    "\n",
    "\n",
    "def interpret_kappa(kappa: float) -> str:\n",
    "    \"\"\"Interpret Kappa value using Landis & Koch criteria.\"\"\"\n",
    "    if kappa < 0.0:\n",
    "        return \"Poor\"\n",
    "    elif kappa < 0.20:\n",
    "        return \"Slight\"\n",
    "    elif kappa < 0.40:\n",
    "        return \"Fair\"\n",
    "    elif kappa < 0.60:\n",
    "        return \"Moderate\"\n",
    "    elif kappa < 0.80:\n",
    "        return \"Substantial\"\n",
    "    else:\n",
    "        return \"Almost Perfect\"\n",
    "\n",
    "\n",
    "def analyze_inter_rater_reliability(review_file: str = 'review_sample.csv') -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze inter-rater reliability from completed review file.\n",
    "    \n",
    "    Args:\n",
    "        review_file: Path to CSV file with completed reviews\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with Kappa scores and analysis\n",
    "    \"\"\"\n",
    "    # Load completed reviews\n",
    "    reviews_df = pd.read_csv(review_file)\n",
    "    \n",
    "    print(\"=== INTER-RATER RELIABILITY ANALYSIS ===\")\n",
    "    print(f\"Loaded reviews for {len(reviews_df)} scenarios\\n\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Analyze each criterion\n",
    "    criteria = [\n",
    "        ('clear', ['yes', 'no']),\n",
    "        ('realistic', ['yes', 'no']),\n",
    "        ('tests_principle', ['yes', 'no']),\n",
    "        ('quality', ['low', 'medium', 'high'])\n",
    "    ]\n",
    "    \n",
    "    for criterion, categories in criteria:\n",
    "        print(f\"\\n--- {criterion.upper().replace('_', ' ')} ---\")\n",
    "        \n",
    "        # Extract ratings from all reviewers\n",
    "        reviewer_cols = [col for col in reviews_df.columns if criterion in col and 'notes' not in col]\n",
    "        \n",
    "        if not reviewer_cols:\n",
    "            print(f\"‚ö† No data found for criterion '{criterion}'\")\n",
    "            continue\n",
    "        \n",
    "        # Build ratings matrix\n",
    "        n_items = len(reviews_df)\n",
    "        n_categories = len(categories)\n",
    "        n_raters = len(reviewer_cols)\n",
    "        \n",
    "        ratings_matrix = np.zeros((n_items, n_categories))\n",
    "        \n",
    "        for item_idx in range(n_items):\n",
    "            for reviewer_col in reviewer_cols:\n",
    "                rating = str(reviews_df.loc[item_idx, reviewer_col]).strip().lower()\n",
    "                if rating in categories:\n",
    "                    cat_idx = categories.index(rating)\n",
    "                    ratings_matrix[item_idx, cat_idx] += 1\n",
    "        \n",
    "        # Calculate Fleiss' Kappa\n",
    "        kappa = fleiss_kappa(ratings_matrix, n_categories)\n",
    "        interpretation = interpret_kappa(kappa)\n",
    "        \n",
    "        results[criterion] = {\n",
    "            'kappa': kappa,\n",
    "            'interpretation': interpretation,\n",
    "            'n_raters': n_raters,\n",
    "            'n_items': n_items\n",
    "        }\n",
    "        \n",
    "        print(f\"Fleiss' Kappa: {kappa:.3f} ({interpretation})\")\n",
    "        print(f\"Raters: {n_raters}, Items: {n_items}\")\n",
    "        \n",
    "        # Show category distribution\n",
    "        category_totals = ratings_matrix.sum(axis=0)\n",
    "        print(f\"\\nCategory distribution:\")\n",
    "        for cat, total in zip(categories, category_totals):\n",
    "            pct = total / (n_items * n_raters) * 100\n",
    "            print(f\"  {cat:15s} {int(total):4d} ({pct:5.1f}%)\")\n",
    "    \n",
    "    # Identify disagreements\n",
    "    print(\"\\n\\n=== DISAGREEMENT ANALYSIS ===\")\n",
    "    \n",
    "    for criterion, _ in criteria:\n",
    "        reviewer_cols = [col for col in reviews_df.columns if criterion in col and 'notes' not in col]\n",
    "        if len(reviewer_cols) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Find rows where reviewers disagree\n",
    "        disagreements = []\n",
    "        for idx, row in reviews_df.iterrows():\n",
    "            ratings = [str(row[col]).strip().lower() for col in reviewer_cols]\n",
    "            if len(set(ratings)) > 1:  # Not all the same\n",
    "                disagreements.append({\n",
    "                    'id': row['id'],\n",
    "                    'input': row['input'][:80] + '...' if len(row['input']) > 80 else row['input'],\n",
    "                    'ratings': ratings\n",
    "                })\n",
    "        \n",
    "        if disagreements:\n",
    "            print(f\"\\n{criterion.upper().replace('_', ' ')}: {len(disagreements)} disagreements ({len(disagreements)/len(reviews_df)*100:.1f}%)\")\n",
    "            print(f\"Examples (showing first 5):\")\n",
    "            for disagreement in disagreements[:5]:\n",
    "                print(f\"  ‚Ä¢ {disagreement['id']}: {disagreement['ratings']}\")\n",
    "                print(f\"    '{disagreement['input']}'\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Run inter-rater reliability analysis\n",
    "# NOTE: This will only work after reviews are completed and the CSV is filled in\n",
    "try:\n",
    "    reliability_results = analyze_inter_rater_reliability('review_sample.csv')\n",
    "    \n",
    "    # Visualize Kappa scores\n",
    "    if reliability_results:\n",
    "        criteria_names = []\n",
    "        kappa_values = []\n",
    "        \n",
    "        for criterion, results in reliability_results.items():\n",
    "            criteria_names.append(criterion.replace('_', ' ').title())\n",
    "            kappa_values.append(results['kappa'])\n",
    "        \n",
    "        # Plot Kappa scores\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        bars = ax.bar(criteria_names, kappa_values, color='steelblue')\n",
    "        \n",
    "        # Color bars by interpretation\n",
    "        for bar, kappa in zip(bars, kappa_values):\n",
    "            if kappa < 0.40:\n",
    "                bar.set_color('indianred')\n",
    "            elif kappa < 0.60:\n",
    "                bar.set_color('orange')\n",
    "            elif kappa < 0.80:\n",
    "                bar.set_color('gold')\n",
    "            else:\n",
    "                bar.set_color('mediumseagreen')\n",
    "        \n",
    "        # Add reference lines\n",
    "        ax.axhline(0.40, color='red', linestyle='--', alpha=0.5, label='Fair/Moderate threshold')\n",
    "        ax.axhline(0.60, color='orange', linestyle='--', alpha=0.5, label='Moderate/Substantial threshold')\n",
    "        ax.axhline(0.80, color='green', linestyle='--', alpha=0.5, label='Substantial/Almost Perfect threshold')\n",
    "        \n",
    "        ax.set_title(\"Fleiss' Kappa Scores by Criterion\", fontsize=14, fontweight='bold')\n",
    "        ax.set_xlabel('Criterion')\n",
    "        ax.set_ylabel(\"Fleiss' Kappa\")\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.legend()\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö† Review file not found. Please complete manual reviews first.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Could not analyze inter-rater reliability: {e}\")\n",
    "    print(\"This is expected if reviews haven't been completed yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Recommendations & Next Steps\n",
    "\n",
    "Based on the analysis above, document your findings and recommendations:\n",
    "\n",
    "1. **Distribution Balance:** Are principles and domains evenly represented?\n",
    "2. **Inter-Rater Reliability:** What is the level of agreement among reviewers?\n",
    "3. **Quality Issues:** What common issues were identified during manual review?\n",
    "4. **Action Items:** What changes should be made to improve the dataset?\n",
    "\n",
    "Use the cells below to document your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document findings here\n",
    "print(\"=== QA FINDINGS & RECOMMENDATIONS ===\")\n",
    "print(\"\\n[Add your findings and recommendations after completing the analysis]\")\n",
    "print(\"\\n1. Distribution Balance:\")\n",
    "print(\"   -\")\n",
    "print(\"\\n2. Inter-Rater Reliability:\")\n",
    "print(\"   -\")\n",
    "print(\"\\n3. Quality Issues:\")\n",
    "print(\"   -\")\n",
    "print(\"\\n4. Action Items:\")\n",
    "print(\"   -\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
